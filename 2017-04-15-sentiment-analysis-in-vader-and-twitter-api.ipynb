{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Applying sentiment analysis with VADER and the Twitter API\n",
    "date: 2017-04-15\n",
    "comments: false\n",
    "tags: python, programming tips, text mining\n",
    "keywords: python, data science, text mining, machine learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few months ago, I posted a blog post about a small project I did where I analysed how people felt about the New Year's resolutions they post on Twitter. In this post, we'll go through the under-the-hood details of how I carried out this analysis, as well as some of the issues I encountered that are pretty typical of a text mining project.\n",
    "\n",
    "If you're interested in getting a bit more detail on the package I used to do the sentiment analysis, VADER, you can see this in last week's analysis. If not, let's jump straight into it!\n",
    "\n",
    "## Setting up your app\n",
    "\n",
    "To do this analysis, I pulled data from [Twitter's public search API](https://dev.twitter.com/rest/public/search), which allows you to pull historical results from up to a week ago. To get started, you will need to create a Twitter account (if you don't already have one), and then jump over to Twitter's [application management portal](https://apps.twitter.com/). If you've never done this before, what we are doing here is creating a unique 'identity' that will allow Twitter to work out who we are when we're accessing their public API. This is a way for them to boot off users or apps that are using the API too heavily or doing dodgy stuff like spamming the site.\n",
    "\n",
    "Once in there, hit the 'Create New App' button, and you'll be prompted to enter a name, description and website for your app. It doesn't really matter what you write in here - just make sure that the name is not so generic that you can distinguish one app from another.\n",
    "\n",
    "<img src=\"/figure/Vader_3.png\" title=\"Create your application\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "Once you've done that, you'll want to jump into the 'Keys and Access Tokens' tab. There are 4 bits of information we need to get from here so that our Python program can connect to the API. We need the consumer key and the consumer secret (circled at the top of the below screenshot), and also the access token and the access token secret (circled at the bottom). As you can see I have blurred mine out - you should take care to keep these secure and not do something like commit them to a public Github repo or anything (definitely not something I've done in the past...).\n",
    "\n",
    "<img src=\"/figure/Vader_4.png\" title=\"Get your keys\" style=\"display: block; margin: auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling down some data\n",
    "\n",
    "Now that we have our keys, we can connect to the API and pull down some data. In order to do this, we first need to install and import the `tweepy` and `json` packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take those keys that we got from the app and use them to set up the connection to the API. As you can see below, we need to pass these keys to the authorisation handler and then get the API method from tweepy to use them. We also need to get tweepy to return the results as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enter authorisations\n",
    "consumer_key = \"uotv8mB6Zlr54p4sTI6N2HFgz\"\n",
    "consumer_secret = \"XjCg5YEQmBdNoqqBcgM3R84eLAYncOczbwwOQQOkoeO8TrDe8D\"\n",
    "access_key = \"89087163-68jLOHDBmtb12GTtWQd6qyFtJ5JgkxtqIgoWI9NGr\"\n",
    "access_secret = \"IVYuHEYVHIkvCw2S2rNIhSXP4mgWvizDnaGF9nh4NK9OA\"\n",
    "\n",
    "# Set up your authorisations\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "\n",
    "# Set up API call\n",
    "api = tweepy.API(auth, parser = tweepy.parsers.JSONParser())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've done that, let's define our search. We need to restrict our search to the exact phrase \"new year's resolution\", and I also want to get rid of retweets (because they are essentially just duplicates in this dataset). The full list of possible ways to search are in the [search API documentation](https://dev.twitter.com/rest/public/search), and they are surprisingly flexible - in fact you can even search on sentiment in your query! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set search query\n",
    "searchquery = '\"new years resolution\" -filter:retweets'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make our call to the API. You can see here I've limited my search to my specific query and English-language results. I'm also limiting the search to 100 tweets, which is the maximum you can return in a single call (we'll get to how we get some more volume soon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = api.search(q = searchquery, count = 100, lang = 'en', result_type = 'mixed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at this data, which is JSON format. For those of you who haven't worked with JSON before, in order to get our data out, we just need to find how to reference it properly in the structure, which is a series of nested Python lists and dictionaries. (I've written in more detail on how to work your way through a JSON file [here]({filename}2015-11-25-reddit-api-part-2.md)). In our case, all of the data about each tweet is contained in a dictionary. Each dictionary is contained in a list, and this list is contained at index 1 of an overarching list. Thus the below code returns the tweet text for tweet number 12 in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'my new years resolution is to be even more bitter than before'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.values()[1][12]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting some volume\n",
    "\n",
    "Now that we've returned our first 100 tweets, we need to scale up to get enough tweets to actually analyse. In order to do this, we need to put our original API call into a loop. However, we need each loop to start after the final tweet returned by the previous call. To do this, we extract the ID of the last tweet from each call and add this to the `max_id` argument in the `api.search()` method.\n",
    "\n",
    "In order to make sure we're not exceeding the number of API calls we can make, we can rate-limit our calls using the `sleep()` method from the time package. You can see I've put 4 seconds between calls.\n",
    "\n",
    "Finally, you can see I've stripped the results our of that outer list, and appended them to a list called data_all. We'll use this list as the basis of our DataFrame in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "[{u'message': u'Rate limit exceeded', u'code': 88}]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-89d2bbdb6743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearchquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mixed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdata_all\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jodieburchell/.virtualenvs/twitter/lib/python2.7/site-packages/tweepy/binder.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;31m# Set pagination mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jodieburchell/.virtualenvs/twitter/lib/python2.7/site-packages/tweepy/binder.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_rate_limit_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: [{u'message': u'Rate limit exceeded', u'code': 88}]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "data = api.search(q = searchquery, count = 100, lang = 'en', result_type = 'mixed')\n",
    "data_all = data.values()[1]\n",
    "\n",
    "while (len(data_all) <= 20000):\n",
    "    time.sleep(4)\n",
    "    last = data_all[-1]['id']\n",
    "    data = api.search(q = searchquery, count = 100, lang = 'en', result_type = 'mixed', max_id = last)\n",
    "    data_all += data.values()[1][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it in a DataFrame\n",
    "\n",
    "We now have a list of up to 20,000 dictionaries containing all of the metadata about each tweet (I say up to, as your particular query may not have enough matches from the past week). We now want to pull out specific information about each tweet, as well as generate our sentiment metrics. \n",
    "\n",
    "For my particular analysis, I used the tweet text and the number of favourites each tweet received, but feel free to play and explore the huge amount of metadata you get back about each tweet for your own purposes - it's honestly a bit creepy how much data you can readily access!\n",
    "\n",
    "The first "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorising our tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing the analyses\n",
    "\n",
    "I think I should do a link out to a gist with details on the graphs I created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some issues with this analysis\n",
    "\n",
    "Text mining traps that come up when you don't carefully clean your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
