{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# English texts\n",
    "engTexts = [u'truth universally acknowledge single man possession good fortune must want wife',\n",
    "            u'angry man drag father along ground orchard stop cry groan old man last stop I drag father beyond tree',\n",
    "            u'old man fish alone skiff gulf stream go day now without take fish',\n",
    "            u'know without read book name adventure tom sawyer matter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spanish texts\n",
    "espTexts = [u'lugar mancha cuyo nombre querer acordarme hacer tiempo vivir hidalgo lanzar astillero adarga antiguo roc\\xedn flaco galgo corredor',\n",
    "            u'a\\xf1o despu\\xe9s frente pelot\\xf3n fusilamiento coronel aureliano buend\\xeda haber recordar aquella tarde remoto padre llevar conocer hielo',\n",
    "            u'bastar\\xe1 decir ser juan pablo castel pintor matar mar\\xeda iribarne suponer proceso recuerdo necesitar mayor explicaci\\xf3n persona',\n",
    "            u'se\\xf1or ser malo aunque faltar\\xedan motivo serlo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with mojibake\n",
    "\n",
    "[Mojibake??](https://en.wikipedia.org/wiki/Mojibake) What the heck is that?? It is a very cute term for that very annoying thing that happens when your text gets changed from one form of encoding to another and your special characters and punctuation turn into that crazy character salad. (In fact, the German term for this, *Buchstabensalat* means 'letter salad'.) As we've already noticed, this has happened with all of the special characters (like á and ñ) in our Spanish sentences.\n",
    "\n",
    "The good news is that it is pretty easy to reclaim our special characters. However, the **bad** news is that we need to jump over to Python 3 to do so. We can use a Python 3 package called [ftfy](https://github.com/LuminosoInsight/python-ftfy), or 'fixes text for you', which is designed to deal with these encoding issues. Let's go ahead and install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip3 install ftfy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `fix_encoding()` function to get rid of all of that ugly mojibake. Let's see how it goes with our first line of Spanish text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lugar mancha cuyo nombre querer acordarme hacer tiempo vivir hidalgo lanzar astillero adarga antiguo rocín flaco galgo corredor\n"
     ]
    }
   ],
   "source": [
    "import ftfy\n",
    "\n",
    "print(ftfy.fix_encoding(espTexts[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! It has worked beautifully, with the 'í' put back into 'rocín'. Now we can fix up all of our text in preparation for the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'año después frente pelotón fusilamiento coronel aureliano buendía haber recordar aquella tarde remoto padre llevar conocer hielo'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "espTexts = [ftfy.fix_encoding(sentence) for sentence in espTexts]\n",
    "espTexts[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenising the text and getting the frequencies\n",
    "\n",
    "We have finally cleaned this text to a point where we can tokenise it and get the frequencies of all of the words. This is very straightforward in NLTK - we simply use the the `word_tokenize` function from the [tokenize package](http://www.nltk.org/api/nltk.tokenize.html). We'll import it below and run it over our lists of English and Spanish text separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['truth', 'universally', 'acknowledge', 'single', 'man', 'possession', 'good', 'fortune', 'must', 'want', 'wife'], ['angry', 'man', 'drag', 'father', 'along', 'ground', 'orchard', 'stop', 'cry', 'groan', 'old', 'man', 'last', 'stop', 'I', 'drag', 'father', 'beyond', 'tree'], ['old', 'man', 'fish', 'alone', 'skiff', 'gulf', 'stream', 'go', 'day', 'now', 'without', 'take', 'fish'], ['know', 'without', 'read', 'book', 'name', 'adventure', 'tom', 'sawyer', 'matter']]\n"
     ]
    }
   ],
   "source": [
    "engTokens = [word_tokenize(text) for text in engTexts]\n",
    "print(engTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['lugar', 'mancha', 'cuyo', 'nombre', 'querer', 'acordarme', 'hacer', 'tiempo', 'vivir', 'hidalgo', 'lanzar', 'astillero', 'adarga', 'antiguo', 'rocín', 'flaco', 'galgo', 'corredor'], ['año', 'después', 'frente', 'pelotón', 'fusilamiento', 'coronel', 'aureliano', 'buendía', 'haber', 'recordar', 'aquella', 'tarde', 'remoto', 'padre', 'llevar', 'conocer', 'hielo'], ['bastará', 'decir', 'ser', 'juan', 'pablo', 'castel', 'pintor', 'matar', 'maría', 'iribarne', 'suponer', 'proceso', 'recuerdo', 'necesitar', 'mayor', 'explicación', 'persona'], ['señor', 'ser', 'malo', 'aunque', 'faltarían', 'motivo', 'serlo']]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "espTokens = [word_tokenize(text) for text in espTexts]\n",
    "espTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to do a very simple frequency count of all of the words in each of the language's texts, using the `FreqDist` function from `nltk`. Let's import the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the tokenised list of words, we need to flatten it. We can then run the `FreqDist` method over it and get the top 10 results for each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man: 4\n",
      "drag: 2\n",
      "father: 2\n",
      "stop: 2\n",
      "old: 2\n",
      "fish: 2\n",
      "without: 2\n",
      "truth: 1\n",
      "universally: 1\n",
      "acknowledge: 1\n"
     ]
    }
   ],
   "source": [
    "flatList = [word for sentList in engTokens for word in sentList]\n",
    "engFreq = FreqDist(word for word in flatList)\n",
    "\n",
    "for word, frequency in engFreq.most_common(10):\n",
    "    print(u'{}: {}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser: 2\n",
      "lugar: 1\n",
      "mancha: 1\n",
      "cuyo: 1\n",
      "nombre: 1\n",
      "querer: 1\n",
      "acordarme: 1\n",
      "hacer: 1\n",
      "tiempo: 1\n",
      "vivir: 1\n"
     ]
    }
   ],
   "source": [
    "flatList = [word for sentList in espTokens for word in sentList]\n",
    "espFreq = FreqDist(word for word in flatList)\n",
    "\n",
    "for word, frequency in espFreq.most_common(10):\n",
    "    print(u'{}: {}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! This is obviously not the most useful metric (as we only have 4 sentences in each corpora), but you can see that we've arrived at something that, with more data, would form a solid foundation for a bag-of-words analysis. You can also see that while it is a bit of work to strip a text down to a useable form, there are plenty of Python packages to make this work pretty painless, even if you're working across a number of languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
